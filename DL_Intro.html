<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-S58CL6FW7Y"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
  
      gtag('config', 'G-S58CL6FW7Y');
    </script>
      <!-- Materialize - Compiled and minified CSS-->
      <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/materialize/0.95.3/css/materialize.min.css" />
      <!-- Font Awesome Icon - CSS-->
      <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" />
      <!-- Custom Styles-->
      <link rel="stylesheet" href="assets/css/blog.css" />
      <title>Blog | DL Intro</title>
  </head>
  
  <div class="header">
      <h2><span class="page-title white-text teal">Deep Learning Part 0: Introduction</span></h2>
  </div>
    
    <div class="row">
      <div class="leftcolumn">
        <div class="card">
          <h2 class="page-title white-text teal">OUTLINE</h2>
          <div class="fakeimg"> 
              <img alt="Motivation sign" style="height:200px;" 
              src="assets/img/blog/motivation/Motivation_Logo.jpg"/>
            <p>This part of the blog will focus on reviewing Machine Learning (ML)
                fundamentals, specifically:
            </p>
            <li>Components of Learning</li>
            <li>Perceptron and Neural Networks</li>
            <li>Gradient Descent and Backpropagation</li>
            <li>Learning Curves</li>
            <li>Regularisation</li>
          </div>
      </div>
  
    <div class="card">
      <h4 class="page-title white-text teal">Components of Learning</h4>
              <p>The map below shows the components of learning and how they
                  relate to allow a ML model to "learn".
              </p>
              <img alt="Components of Learning" style="height:100px;" 
              src="assets/img/blog/ICL_DL/DL_Part_0/ML_Components_Of_Learning.PNG"/>
              <p>We define the Domain to be the dimensionality of the data <b>x</b>, which are
                  also called its' features. For supervised learning, we have true output labels,
                  <b>y</b> defined for each input, <b>x</b>. We feed the input and output labels
                  (<b>x</b>, <b>y</b>) into the model, often in batches, to learn from. This is
                  shown by the Learners arrow. The target function, <b>f</b>, is unknown and
                  it is the goal of the ML model to learn this function, i.e. to get
                  <b>g --> f</b>.
              </p>
              <p>One way to describe a model is through its' hypothesis class, <b>H</b>, which 
                defines the set of possible configurations a model can choose from. 
                Thus it can be seen as a way to limit how well (or too well!) a model can 
                reach its goal as defined above. It is hoped that the model will choose 
                the best configuration to reach this goal through learning. 
              </p>
              <p>The error function and error minimisation are the primary ways an ML model
                learns. For each task and scenario, we choose these to give the so that it can
                track how well it is learning (error function) and optimise itself (error minimisation) 
                to reach its goal.
              </p>
    </div>

    <div class="card">
        <h4 class="page-title white-text teal">Perceptron and Neural Networks</h4>
          <p>The perceptron, pictured below, was insipred by the biological neuron found
              in the brain. It consists of weights, <b>w</b>, multiplied by a data point, <b>x</b>,
              through an activiation function, theta(<b>w^Tx</b>), to its output <b>y_hat</b>.
          </p>
          <img alt="Perceptron" style="height:100px;" 
          src="assets/img/blog/ICL_DL/DL_Part_0/Perceptron"/>
          <p>The most basic neural network is essentially a stacked version of the perceptron, i.e.
              a multi-layer perceptron model with activation functions at each node. The structure
              is shown in the image below. 
          </p>
          <img alt="Neural Network" style="height:100px;" 
          src="assets/img/blog/ICL_DL/DL_Part_0/Neural_Network.PNG"/>
          <p>We can see that each node is connected to all of its previous layer nodes. One layer
              is the veritcal combination of nodes, where the first layer is the input data <b>x</b>. 
              Each line represents the multiplacation of a weight, <b>w_ij</b> and the output of the 
              previous node <b>s_ij</b> (the i and j notation is not shown in the diagram, but let a 
              layer be defined by i and vertical location of node be j.)
          </p>
          <p>The "forward pass" refers to the calculation of the outputs of each node, from left
              to right. It is the way an input data point is mapped to an output. The dimension of the output
              depends on the task; the above picture has an output dimensionality D_h = 1 which is commonly
              used in binary classification or regression tasks. The forward pass is essentialy a scaled up
              version of the perceptron description in the first paragraph. A worked example of a
              forward pass on an MLP is shown below. 
          </p>
          <img alt="Forward Pass" style="height:100px;" 
          src="assets/img/blog/ICL_DL/DL_Part_0/Forward_Pass.PNG"/>
    </div>

    <div class="card">
      <h4 class="page-title white-text teal">Gradient Descent and Backpropagation</h4>
        <p>Gradient descent is a general method for non-linear optimization. It is often used
            in ML when a model doesn't have a closed form solution, for example in 
            neural networks. It is the fundamental way in which models optimise to reach their goal,
            and forms a part of the wider backpropagation algorithm. Gradient descent in its 
            simplest form is formulated as: <b>w_{t+1} = w_t + mui*v</b>, where <b>v</b> is the
            normalised direction for finding the local minimum error and <b>mui</b> is the learning rate.
            The learning rate affects how many steps it takes to find the local error minimum.
            The graphs below show how different learning rates impact the speed of convergence.
            </p>
            <img alt="Forward Pass" style="height:100px;" 
            src="assets/img/blog/ICL_DL/DL_Part_0/Learning_Rate_Effect.PNG"/>
        <p>Backpropagation is the fundamental way neural networks learn and comprosises of all the 
            discussed topics. The algorithm is as follows:
        </p>
        <ol>
            <li>Initialise weights <b>w_ij</b> at random</li>
            <li>For t = 1,2...K:</li>
            <ol>
                <li>Forward pass: Compute all <b>x_j</b></li>
                <li>Backward pass: Compute all <b>delta_j</b></li>
                <li>Gradient Descent: <b>w_ij = w_ij - mui*x_y*delta_j</b></li>
            </ol>
        <li>Return final weights <b>w_ij</b></li>
        </ol>
    </div>

    <div class="card">
      <h4 class="page-title white-text teal">Learning Curves</h4>
        <p>Underfitting and overfitting are problems associated with ML models
            and their affect can be shown with learning curves. Underfitting is
            when a model has not learnt all the features it needs to reach its
            goal of approximating the underlying function. The most prominent reasons
            for underfitting are:
            <li>Not enough training time (in epochs)</li>
            <li>Not enough complexity in the model</li>
        </p>
        <p>Overfititng occurs when the model begins to learn the noise of the target
            function instead of the features. The most prominent reasons for overfitting is:
            <li>Not enough data points</li>
            <li>Too much training time</li>
            <li>Too high model complexity</li>
        </p>
        <p>Thus, the optimal point at which to stop training the model, i.e. stop learning,
            is at the intersection of underfitting and overfitting. This can be measured 
            through the point at which the test/validation error begins to increase but the
            training error decreases. This is shown in the learning curves below.
        </p>
        <img alt="Forward Pass" style="height:100px;" 
        src="assets/img/blog/ICL_DL/DL_Part_0/One_H.PNG"/>
        <img alt="Forward Pass" style="height:100px;" 
        src="assets/img/blog/ICL_DL/DL_Part_0/Many_H.PNG"/>
    </div>

    <div class="card">
      <h4 class="page-title white-text teal">Regularisation</h4>
        <p>A popular tool to aid in ML model learning is regularisation. Regularisation
            enforces an "overfit penalty" on the error function in an attempt to 
            guide the model to the optimal point on the learning curve as described previously.
        </p>
        <p>There are various regularisation techniques, for example L2-Loss restricts the 
            possible <b>w </b> weights to within a circle of values. However, they are all based
            on a fundamental premise: Occam's Razor. Occam's Razor essentially states that
            simpler is usually better. This is based on the scientific notion that
            noise is not smooth, and an ML model's learning seeks to ignore noise. 
            Regularisation Thus incentivises the model's learnt function towards smoother, simpler functions.
        </p>
        <p>Regularisation introduces another(s) hyperparameter, <b>lambda</b>, to the model. This hyperparameter
            controls how much weighting the model attributes to regularisation. The effect of 
            different <b>lambda</b> values on learning is shown below.
        </p>
        <img alt="Forward Pass" style="height:100px;" 
        src="assets/img/blog/ICL_DL/DL_Part_0/Regularisation.PNG"/>
    </div>
    </div>
</div>
  
<div class="rightcolumn">
    <div class="card">
      <h2 class="page-title white-text teal">ABOUT</h2>
      <div class="fakeimg">
      <p>Hosted by Dr. Andrew Huberman, The Huberman Lab Podcast discusses science 
          and science-based tools for everyday life. On this episode, the idea of motivation
          and drive is discussed, and in particular tools to increase motivation and drive
          to achieve and sustain goals.
      </p>
      <img alt="Huberman Lab" style="height:200px;" src="assets/img/blog/Huberman_Lab.PNG"/>
  </div>
  </div>

    <div class="card">
      <h4 class="page-title white-text teal">Other Blog Posts</h4>
      <div class="fakeimg">
        <img alt="Motivation sign" style="height:200px;" href="https://horsada.github.io/Horsada_Portfolio/productivity"
      src="assets/img/blog/productivity/Productivity_Logo.jpg">
      </div><br>
    </div>

    <div class="card">
      <h3 class="page-title white-text teal">Follow Me</h3>
      <p>Some text..</p>
    </div>
  </div>
</div>

<div class="footer">
  <h2>Footer</h2>
</div>