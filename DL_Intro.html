<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-S58CL6FW7Y"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
  
      gtag('config', 'G-S58CL6FW7Y');
    </script>
      <!-- Materialize - Compiled and minified CSS-->
      <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/materialize/0.95.3/css/materialize.min.css" />
      <!-- Font Awesome Icon - CSS-->
      <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" />
      <!-- Custom Styles-->
      <link rel="stylesheet" href="assets/css/blog.css" />
      <title>Blog | Motivation</title>
  </head>
  
  <div class="header">
      <h2><span class="page-title white-text teal">Deep Learning Part 0: Introduction</span></h2>
  </div>
    
    <div class="row">
      <div class="leftcolumn">
        <div class="card">
          <h2 class="page-title white-text teal">OUTLINE</h2>
          <div class="fakeimg"> 
              <img alt="Motivation sign" style="height:200px;" 
              src="assets/img/blog/motivation/Motivation_Logo.jpg"/>
            <p>This part of the blog will focus on reviewing Machine Learning (ML)
                fundamentals, specifically:
            </p>
            <li>Components of Learning</li>
            <li>Perceptron and Neural Networks</li>
            <li>Gradient Descent and Backpropagation</li>
            <li>Learning Curves</li>
            <li>Regularisation</li>
          </div>
      </div>
  
    <div class="card">
      <h4 class="page-title white-text teal">Components of Learning</h4>
              <p>The map below shows the components of learning and how they
                  relate to allow a ML model to "learn".
              </p>
              <img alt="Components of Learning" style="height:100px;" 
              src="assets/img/blog/ICL_DL/DL_Part_0/ML_Components_Of_Learning.PNG"/>
              <p>We define the Domain to be the dimensionality of the data <b>x</b>, which are
                  also called its' features. For supervised learning, we have true output labels,
                  <b>y</b> defined for each input, <b>x</b>. We feed the input and output labels
                  (<b>x</b>, <b>y</b>) into the model, often in batches, to learn from. This is
                  shown by the Learners arrow. The target function, <b>f</b>, is unknown and
                  it is the goal of the ML model to learn this function, i.e. to get
                  <b>g --> f</b>.
              </p>
              <p>One way to describe a model is through its' hypothesis class, <b>H</b>, which 
                defines the set of possible configurations a model can choose from. 
                Thus it can be seen as a way to limit how well (or too well!) a model can 
                reach its goal as defined above. It is hoped that the model will choose 
                the best configuration to reach this goal through learning. 
              </p>
              <p>The error function and error minimisation are the primary ways an ML model
                learns. For each task and scenario, we choose these to give the so that it can
                track how well it is learning (error function) and optimise itself (error minimisation) 
                to reach its goal.
              </p>
    </div>

    <div class="card"></div>
        <h4 class="page-title white-text teal">Perceptron and Neural Networks</h4>
          <p>The perceptron, pictured below, was insipred by the biological neuron found
              in the brain. It consists of weights, <b>w</b>, multiplied by a data point, <b>x</b>,
              through an activiation function, <b>theta(<b>w^Tx</b>), to its output <b>y_hat</b>.
          </p>
          <img alt="Perceptron" style="height:100px;" 
          src="assets/img/blog/ICL_DL/DL_Part_0/Perceptron"/>
          <p>The most basic neural network is essentially a stacked version of the perceptron, i.e.
              a multi-layer perceptron model with activation functions at each node. The structure
              is shown in the image below. 
          </p>
          <img alt="Neural Network" style="height:100px;" 
          src="assets/img/blog/ICL_DL/DL_Part_0/Neural_Network.PNG"/>
          <p>We can see that each node is connected to all of its previous layer nodes. One layer
              is the veritcal combination of nodes, where the first layer is the input data <b>x</b>. 
              Each line represents the multiplacation of a weight, <b>w_ij</b> and the output of the 
              previous node <b>s_ij</b> (the i and j notation is not shown in the diagram, but let a 
              layer be defined by i and vertical location of node be j.)
          </p>
          <p>The "forward pass" refers to the calculation of the outputs of each node, from left
              to right. It is the way an input data point is mapped to an output. The dimension of the output
              depends on the task; the above picture has an output dimensionality D_h = 1 which is commonly
              used in binary classification or regression tasks. The forward pass is essentialy a scaled up
              version of the perceptron description in the first paragraph. A worked example of a
              forward pass on an MLP is shown below. 
          </p>
          <img alt="Forward Pass" style="height:100px;" 
          src="assets/img/blog/ICL_DL/DL_Part_0/Forward_Pass.PNG"/>
    </div>

    <div class="card">
      <h4 class="page-title white-text teal">Gradient Descent and Backpropagation</h4>
        <p></p>
    </div>

    <div>
      <h4 class="page-title white-text teal">Learning Curves</h4>
        <p>
        </p>
    </div>

    <div class="card">
      <h4 class="page-title white-text teal">Regularisation</h4>
        <p>
        </p>
    </div>
    </div>
</div>
  
      <div class="rightcolumn">
        <div class="card">
          <h2 class="page-title white-text teal">ABOUT</h2>
          <div class="fakeimg">
          <p>Hosted by Dr. Andrew Huberman, The Huberman Lab Podcast discusses science 
              and science-based tools for everyday life. On this episode, the idea of motivation
              and drive is discussed, and in particular tools to increase motivation and drive
              to achieve and sustain goals.
          </p>
          <img alt="Huberman Lab" style="height:200px;" src="assets/img/blog/Huberman_Lab.PNG"/>
      </div>
      </div>
        <div class="card">
          <h3 class="page-title white-text teal">Other Blog Posts</h3>
          <div class="fakeimg">Image</div><br>
          <div class="fakeimg">Image</div><br>
          <div class="fakeimg">Image</div>
        </div>
        <div class="card">
          <h3 class="page-title white-text teal">Follow Me</h3>
          <p>Some text..</p>
        </div>
      </div>
    </div>
    
    <div class="footer">
      <h2>Footer</h2>
    </div>